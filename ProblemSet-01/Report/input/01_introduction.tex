\chapter{Implementation}
\label{implementation}
%#############################################################################################
%######################           Motivation          ########################################
%#############################################################################################

This chapter explains the implementation of some algorithms that we will use in this problem set. It includes: \textit{PCA}, \textit{$\gamma$-index} and \textit{LLE}.

\section{Assignment 1: PCA}
\label{sec:assignment1}

In this assignment we have to implement the function \textit{pca} which receives a \textit{d x n} matrix \textit{X} and the number of components \textit{m} as parameters, and returns the principal components as well as the projected data points in a \textit{m x n} matrix \textit{Z}. Furthermore, the function should also return a \textit{d x d} matrix \textit{U} and a \textit{1 x d} vector \textit{D}. The matrix \textit{U} contains the principal directions, whereas the vector \textit{D} contains the principal values, sorted in descending order ($D_1 \geq D_2 ...$).

I have tested the implemented function on the test data and it passed the test. Following steps are performed in the function:
\begin{enumerate}
	\item Substract \textit{X} from its mean.
	\item Calculate the covariance matrix from zero-mean \textit{X}.
	\item Calculate the eigenvectors and eigenvalues from the covariance matrix.
	\item Sort the eigenvalues and eigenvectors in decending order.
	\item Form the feature vectors by taking only the first \textit{m} eigenvectors.
	\item Project the zero-mean \textit{X} to the feature vectors.
\end{enumerate}

%######################################################################################
\section{Assignment 2: $\gamma$-Index}
\label{sec:assignment2}

The $\gamma$-index has to be implemented in this assignment. The function receives a \textit{d x n} matrix \textit{X} containing the data points and the numbers of neighbours \textit{k}, and returns the $\gamma$-index for each data points in the \textit{1 x n} vector \textit{y}.

\citeasnoun**{Harmeling2006} formulate the the formula to calculate the $\gamma$-index for one data point as follows:
$\gamma(x)=\frac{1}{k} \sum_{j=1}^{k} \| x-z_j(x) \|$

The implemented function has passed the test. Following steps are performed in the function:
\begin{enumerate}
	\item Implement a new function \textit{distmat} that calculates the distances from the data points to each other and return the distances as a matrix.
	\item Get the distance matrix using the function \textit{distmat} mentioned above.
	\item Sort the distance matrix in ascending order.
	\item Take only the \textit{k}-nearest data points as neighbours for each data point.
	\item Calculate the mean from the distances of the \textit{k}-nearest neighbours and set it as the $\gamma$-index.
	
\end{enumerate}


%######################################################################################
\section{Assignment 3: LLE}
\label{sec:assignment3}

The last assignment in the implementation part asks us to implement the \textit{locally linear embedding} method as described in \cite{Saul2000}. The \textit{lle} function returns a \textit{m x n} matrix \textit{Y} representing the resulting embedding and takes following parameters as inputs:
\begin{itemize}
	\item A \textit{d x n} matrix \textit{X} containing the data points.
	\item A scalar \textit{m} representing the dimension of the resulting embedding.
	\item A string \textit{n\_rule} determining the method (\textit{'knn'} or \textit{'eps-ball'}) for building the neighbourhood graph.
	\item A scalar \textit{param} used as parameter for the \textit{n\_rule} (\textit{k} or $\epsilon$, respectively).
	\item A scalar \textit{tol} determining the size of the regularization parameter.
\end{itemize}

The implementation is based on the pseudocode described by \citeasnoun{Roweis} on their website, which contains of three main parts:
\begin{enumerate}
	\item Find the nearest neighbours of each data point based on \textit{n\_rule}.
	\item Solve for reconstruction weights \textit{W}.
	\item Compute embedding coordinates \textit{Y} using weights \textit{W}.
\end{enumerate}
