\chapter{Application}
\label{chap:application}
%#############################################################################################

In this chapter, the implementations should be applied to three datasets: \textit{2gaussians} dataset, \textit{5gaussians} and \textit{USPS} dataset.

%#############################################################################################
\section{Assignment 6: \textit{5gaussians} Analysis}
\label{assignment6}

In this assignment, the \textit{5gaussians} data set is analyzed. The data set should be clustered using \textit{k-means} and \textit{GMM} for $k = 2, ..., 10$. Figure~\ref{fig:5gaussians} shows the original \textit{5gaussians} data set.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{5gaussians}
	\caption{\textit{5gaussians} data set}
	\label{fig:5gaussians}
\end{figure}

{\raggedright \textbf{Question 6.1: Do both methods find the five clusters reliably?}} \\
\textit{K-means} finds the five clusters reliably. Since \textit{k-means} can have different result depending on the initialization, it was run 100 times. The loss value from each run was calculated, as depicted in Figure~\ref{fig:lossvalues_kmeans}. The clustering which gives the lowest loss value was then picked and visualized in Figure~\ref{fig:5gaussians_kmeans}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{lossvalues_kmeans}
	\caption{Loss values of k-means, run 100 times}
	\label{fig:lossvalues_kmeans}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{5gaussians_kmeans}
	\caption{k-means clustering with $k=5$, applied to \textit{5gaussians} data set}
	\label{fig:5gaussians_kmeans}
\end{figure}

\textit{GMM} with random initialization also finds the five clusters reliably. The same method as \textit{k-means} was applied to \textit{GMM}. It was also run 100 times, but instead of loss value, the log likelihood of each run was calculated, as shown in Figure~\ref{fig:likelihoods_gmm_random}. The clustering which gives the highest log likelihood was then picked and visualized in Figure~\ref{fig:5gaussians_gmm_random}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{likelihoods_gmm_random}
	\caption{Log likelihood of GMM with random initialization, run 100 times}
	\label{fig:likelihoods_gmm_random}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{5gaussians_gmm_random}
	\caption{GMM clustering with random initialization, $k=5$, applied to \textit{5gaussians} data set}
	\label{fig:5gaussians_gmm_random}
\end{figure}

Finally, \textit{GMM} with \textit{k-means} initialization was utilized. It also finds the five clusters reliably. It was also run 100 times and the log likelihood of each run was calculated. Figure~\ref{fig:likelihoods_gmm_kmeans} and Figure~\ref{fig:5gaussians_gmm_kmeans} depict the log likelihood of each run and the clustering which gives the highest log likelihood, respectively.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{likelihoods_gmm_kmeans}
	\caption{Log likelihood of GMM with k-means initialization, run 100 times}
	\label{fig:likelihoods_gmm_kmeans}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{5gaussians_gmm_kmeans}
	\caption{GMM clustering with k-means initialization, $k=5$, applied to \textit{5gaussians} data set}
	\label{fig:5gaussians_gmm_kmeans}
\end{figure}

As we have seen above, the three methods can find the five clusters reliably, because the \textit{5gaussians} data set is a well-separated data set.

%\newpage
\pagebreak[4]
{\raggedright \textbf{Question 6.2: What role does the initialization of the GMM with a k-means solution play in the number of necessary iterations and the quality of the solution?}}\\

The initialization of the \textit{GMM} with a \textit{k-means} solution plays a significant role in the number of necessary iterations until the convergence is reached. \textit{GMM} depends strongly on the initialization. With \textit{k-means} initialization, the \textit{GMM} converged more quickly than random initialization.

\textit{GMM} was run 100 times with random and k-means initialization to avoid some bad local optima. As we can see from Figure~\ref{fig:number_iterations}, \textit{GMM} with random initialization never converged with the number of iterations below 20, whereas \textit{GMM} with k-means initialization converged mostly only with about eight iterations.

On the other hand, the initialization does not play a significant role in the quality of the solution. Both the random and k-means initialization produce almost the same log likelihoods. Figure~ shows the maximum log likelihoods of both initialization which were run 100 times. Both methods produce almost the same maximum log likelihood, about -666.7011.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4783]{number_iterations}
	\caption{Number of iterations of \textit{GMM} ($k=5$) with random and k-means initialization, run 100 times}
	\label{fig:number_iterations}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.4783]{likelihoods_comparison}
	\caption{Maximum log likelihoods of \textit{GMM} ($k=5$) with random and k-means initialization, run 100 times}
	\label{fig:likelihoods_comparison}
\end{figure}


{\raggedright \textbf{Question 6.3: What does the dendogramm of the hierarchical clustering look like and is it possible to pick a suitable value of \textit{k} from the dendogramm?}}\\

It is possible to pick a suitable value of \textit{k} from the dendogramm plot. Figure~\ref{fig:5gaussians_dendogramm_50clusters} shows the dendogramm plot of \textit{kmeans\_agglo} with initial clustering $k=50$. From the plot, we can see that the clusters can actually be divided into five clusters (marked with yellow ellipses). Before the merge of those five clusters, there is no significant increase of the loss values. But after merging those five clusters, the loss values increase significantly.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.7583]{5gaussians_dendogramm_50clusters}
	\caption{Dendogramm plot of the hierarchical clustering with initial clustering $k=50$}
	\label{fig:5gaussians_dendogramm_50clusters}
\end{figure}

In this case the difference in iterations
seems insignificant. However, as will be seen in the presence of poorly separated data a good choice for our
initial guesses can drastically reduce the number of iterations required to obtain convergence


%#############################################################################################
\section{Assignment 7: \textit{2gaussian} Analysis}
\label{assignment7}

In this assignment the \textit{2gaussians} data set should be analyzed with \textit{k-means} and \textit{GMM}. Figure~\ref{fig:2gaussians} show the original \textit{2gaussians} data set.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{2gaussians}
	\caption{\textit{2gaussians} data set}
	\label{fig:2gaussians}
\end{figure}


%#############################################################################################

\section{Assignment 8: \textit{USPS} Analysis}
\label{assignment8}

In the last assignment of this problem set, \textit{LLE} has to be applied to noisy \textit{flatroll} data set. Two gaussian noise were added to the data set, with variance 0.2 and 1.8, respectively. After that, both noisy data sets should be unrolled using \textit{knn} with a good value of \textit{k} and a value which is obviously too large. Figure~\ref{fig:llenoisy} depicts the two noisy images and their resulting embedding. It can be obtained that the noisy data set with big variance(1.8) is not very good unrolled.


