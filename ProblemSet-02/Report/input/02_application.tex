\chapter{Application}
\label{chap:application}
%#############################################################################################

In this chapter, the implementations should be applied to three datasets: \textit{2gaussians} dataset, \textit{5gaussians} and \textit{USPS} dataset.

%#############################################################################################
\section{Assignment 6: \textit{5gaussians} Analysis}
\label{assignment6}

In this assignment, the \textit{5gaussians} data set is analyzed. The data set should be clustered using \textit{k-means} and \textit{GMM} for $k = 2, ..., 10$. Figure~\ref{fig:5gaussians} shows the original \textit{5gaussians} data set.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{5gaussians}
	\caption{\textit{5gaussians} data set}
	\label{fig:5gaussians}
\end{figure}

{\raggedright \textbf{Question 6.1: Do both methods find the 5 clusters reliably?} \\
\textit{K-means} finds the 5 clusters reliably. Since \textit{k-means} can have different result depending on the initialization, it is run 100 times. The loss value from each run is calculated, as depicted in Figure~\ref{fig:lossvalues_kmeans}. The clustering which gives the lowest loss value is then picked and visualized in Figure~\ref{fig:5gaussians_kmeans}.

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{lossvalues_kmeans}
	\caption{Loss values of k-means, run 100 times}
	\label{fig:lossvalues_kmeans}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[scale=0.5983]{5gaussians_kmeans}
	\caption{k-means clustering with $k=5$, applied to \textit{5gaussians} data set}
	\label{fig:5gaussians_kmeans}
\end{figure}



{\raggedright \textbf{Question 6.2: What role does the initialisation of the GMM with a k-means solution play in the number of necessary iterations and the quality of the solution?}}\\

{\raggedright \textbf{Question 6.3: What does the dendogramm of the hierarchical clustering look like and is it possible to pick a suitable value of \textit{k} from the dendogramm?}}\\

In this case the difference in iterations
seems insignificant. However, as will be seen in the presence of poorly separated data a good choice for our
initial guesses can drastically reduce the number of iterations required to obtain convergence



This assignment asks us to apply \textit{PCA} to the \textit{usps} data set and visualizing the results. The \textit{usps} data set consists of 2007 images with the dimension of \textit{16 x 16}. The images are hand-written digits of zero to nine, which can be viewed as classes. Firstly, i separate the data set according to each digit into ten classes and then applied \textit{PCA} to each class. The \textit{PCA} was applied to the original data set and noisy data set.



%\begin{figure}[h!]
%	\centering
%	\includegraphics[scale=0.4983]{normalpca_0-5}
%	\caption{Principal components of usps original data set for class 0 - 5}
%	\label{fig:pcaOriginal05}
%\end{figure}


%#############################################################################################
\section{Assignment 7: \textit{2gaussian} Analysis}
\label{assignment7}

In this assignment, the $\gamma$-index method is utilized to detect outliers and applied it to the \textit{banana} data set. The positive class of the data set is used as \textit{inliers}, to which the negative class is added as outliers. The $\gamma$-index is then used to detect outliers with contamination rates of 1\%, 5\%, 10\% and 25\% relative to the positive class. Figure~\ref{fig:bananacomplete} shows the complete original data set, both positive and negative class, whereas Figure~\ref{fig:bananacontaminated} shows the contaminated data set.



There are three methods that should be used to detect the outliers: (a) the $\gamma$-index with $k=3$, (b) the $\gamma$-index with $k=10$ and (c) the distance to the mean for each data point. All of the methods are then applied to the four contamination rates mentioned above. After that, the \textit{AUC} (area under the \textit{ROC}) should be calculated. Figure~\ref{fig:gammaboxplots} shows the boxplots that visualize the distribution of the \textit{AUC} values.



The boxplots show that the method using the distance to the mean for each data point performed quite bad, while both of the $\gamma$-index methods performed very well, especially for the data set with lower contamination rates. The $\gamma$-index with $k=10$ performed slightly better than the $\gamma$-index with $k=3$.


%#############################################################################################

\section{Assignment 8: \textit{USPS} Analysis}
\label{assignment8}

In the last assignment of this problem set, \textit{LLE} has to be applied to noisy \textit{flatroll} data set. Two gaussian noise were added to the data set, with variance 0.2 and 1.8, respectively. After that, both noisy data sets should be unrolled using \textit{knn} with a good value of \textit{k} and a value which is obviously too large. Figure~\ref{fig:llenoisy} depicts the two noisy images and their resulting embedding. It can be obtained that the noisy data set with big variance(1.8) is not very good unrolled.


